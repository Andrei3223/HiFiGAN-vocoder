defaults:
  - model: HIFIGan
  - writer: wandb
  - metrics: hifi
  - datasets: one_batch_test
  - dataloader: example
  - transforms: example_only_instance
  - _self_
optimizer_d:
  _target_: torch.optim.AdamW
  lr: 2e-4
lr_scheduler_d:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: 7e-4
  pct_start: 0.1
  steps_per_epoch: ${trainer.epoch_len}
  epochs: ${trainer.n_epochs}
  anneal_strategy: cos
optimizer_g:
  _target_: torch.optim.AdamW
  lr: 2e-4
lr_scheduler_g:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: 7e-4
  pct_start: 0.1
  steps_per_epoch: ${trainer.epoch_len}
  epochs: ${trainer.n_epochs}
  anneal_strategy: cos
generator:
  channels: 80
  hidden: 512
  kernels_u: [16, 16, 4, 4],
  kernels_mrf: [3, 7, 11],
  dilations: [[[1, 1], [3, 1], [5, 1]],
          [[1, 1], [3, 1], [5, 1]],
          [[1, 1], [3, 1], [5, 1]]]
trainer:
  log_step: 1
  n_epochs: 1
  epoch_len: 20
  device_tensors: ["spectrogram"] # which tensors should be on device (ex. GPU)
  resume_from: null # null or path to the checkpoint dir with *.pth and config.yaml
  device: auto # device name or "auto"
  override: True # if True, will override the previous run with the same name
  monitor: "min val_WER_(Argmax)" # "off" or "max/min metric_name", i.e. our goal is to maximize/minimize metric
  save_period: 5 # checkpoint each save_period epochs in addition to the best epoch
  early_stop: ${trainer.n_epochs} # epochs for early stopping
  save_dir: "saved"
  seed: 42



{
  "name": "default_config",
  "n_gpu": 1,
  "arch": {
    "type": "HiFiGAN",
    "args": {
      "generator_config": {
        "in_channels": 80,
        "hid_dim": 512,
        "conv_trans_kernel_sizes": [16, 16, 4, 4],
        "mrf_kernel_sizes": [3, 7, 11],
        "mrf_dilations": [
          [[1, 1], [3, 1], [5, 1]],
          [[1, 1], [3, 1], [5, 1]],
          [[1, 1], [3, 1], [5, 1]]
        ]
      },
      "mpd_config": {
        "periods": [2, 3, 5, 7, 11],
        "kernel_size": 5,
        "stride": 3,
        "channels": [32, 128, 512, 1024]
      },
      "msd_config": {
        "factors": [1, 2, 4],
        "kernel_sizes": [15, 41, 41, 41, 41, 41, 5],
        "strides": [1, 2, 2, 4, 4, 1, 1],
        "groups": [1, 4, 16, 16, 16, 16, 1],
        "channels": [128, 128, 256, 512, 1024, 1024, 1024]
      }
    }
  },
  "data": {
    "train": {
      "batch_size": 5,
      "num_workers": 5,
      "datasets": [
        {
          "type": "LJSpeechDataset",
          "args": {
            "data_path": "data/LJSpeech-1.1/wavs",
            "wav_max_len": 22272
          }
        }
      ]
    }
  },
  "optimizer_d": {
    "type": "AdamW",
    "args": {
      "lr": 2e-4,
      "betas": [0.8, 0.99],
      "weight_decay": 0.01
    }
  },
  "optimizer_g": {
    "type": "AdamW",
    "args": {
      "lr": 2e-4,
      "betas": [0.8, 0.99],
      "weight_decay": 0.01
    }
  },
  "lr_scheduler_d": {
    "type": "ExponentialLR",
    "args": {
      "gamma": 0.9
    }
  },
  "lr_scheduler_g": {
    "type": "ExponentialLR",
    "args": {
      "gamma": 0.9
    }
  },
  "loss": {
    "type": "HiFiGANLoss",
    "args": {
      "mel_spectrogram_multiplier": 45,
      "feature_matching_multiplier": 2
    }
  },
  "metrics": [],
  "trainer": {
    "epochs": 1,
    "save_dir": "saved/",
    "save_period": 2,
    "verbosity": 2,
    "monitor": "min val_loss",
    "early_stop": 100,
    "visualize": "wandb",
    "wandb_project": "nv_project",
    "wandb_run_name": "one_batch_test",
    "len_epoch": 3,
    "grad_norm_clip": 10
  }
}